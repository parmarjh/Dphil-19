{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Sprint_19: Classification of Microorganisms of Sukhna and Dhanas Lakes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDisalNlaYQs"
      },
      "source": [
        "![dphi banner](https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/dphi_banner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq0W0R_SWun_"
      },
      "source": [
        "# **Getting Started Code For [Data Sprint #19](https://dphi.tech/practice/challenge/55) on DPhi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xmeez26YdC2"
      },
      "source": [
        "#### **Author: Manish KC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCfCO8Z90f0c"
      },
      "source": [
        "# Loading Libraries\n",
        "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
        "\n",
        "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).\n",
        "\n",
        "Note: You can import all the libraries that you think will be required or can import it as you go along. \n",
        "\n",
        "Here we are importing two libraries - numpy and pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdgJJlH40_bQ"
      },
      "source": [
        "import numpy as np        # Fundamental package for linear algebra and multidimensional arrays\n",
        "import pandas as pd       # Data analysis and manipultion tool\n",
        "\n",
        "# to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1NzvInd1kx6"
      },
      "source": [
        "# Loading Dataset\n",
        "Pandas module is used for reading files.\n",
        "\n",
        "You can learn more about pandas [here](https://dphi.tech/learn/introduction-to-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5inGNMiR1hKx"
      },
      "source": [
        "# In read_csv() function, we have passed the location to where the files are located in the dphi official github page.\n",
        "mo_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/sukhna_dhanas/train_set_label.csv\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ndV57522Czi"
      },
      "source": [
        "## What do you need to do now?\n",
        "*  Perform EDA and Data Visualization üëÄ to understand the data. Learn more about EDA [here](https://dphi.tech/learn/introduction-to-exploratory-data-analysis). Learn more about data visualization [here](https://dphi.tech/learn/introduction-to-data-visualization)\n",
        "*  Clean the data if required (like removing or filling missing values, treat outliers, etc.). Learn more about handling missing values [here](https://youtu.be/EaGbS7eWSs0)\n",
        "*  Perform Data Preprocessing if you feel it's required. Learn one hot encoding [here](https://youtu.be/9yl6-HEY7_s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIDMrCje8oXu"
      },
      "source": [
        "## Basic EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp6SaZm68qz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "cbd9e4b8-b2b7-40be-8663-8c6af0e216b7"
      },
      "source": [
        "mo_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Solidity</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extrema</th>\n",
              "      <th>FilledArea</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Orientation</th>\n",
              "      <th>EulerNumber</th>\n",
              "      <th>BoundingBox1</th>\n",
              "      <th>BoundingBox2</th>\n",
              "      <th>BoundingBox3</th>\n",
              "      <th>BoundingBox4</th>\n",
              "      <th>ConvexHull1</th>\n",
              "      <th>ConvexHull2</th>\n",
              "      <th>ConvexHull3</th>\n",
              "      <th>ConvexHull4</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>Centroid1</th>\n",
              "      <th>Centroid2</th>\n",
              "      <th>Area</th>\n",
              "      <th>microorganism</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.711636</td>\n",
              "      <td>0.673498</td>\n",
              "      <td>0.109069</td>\n",
              "      <td>0.870544</td>\n",
              "      <td>0.010808</td>\n",
              "      <td>0.660599</td>\n",
              "      <td>0.094353</td>\n",
              "      <td>0.987915</td>\n",
              "      <td>0.870476</td>\n",
              "      <td>0.447276</td>\n",
              "      <td>0.029022</td>\n",
              "      <td>0.058923</td>\n",
              "      <td>0.875395</td>\n",
              "      <td>0.875395</td>\n",
              "      <td>0.877460</td>\n",
              "      <td>0.871746</td>\n",
              "      <td>0.025669</td>\n",
              "      <td>0.028256</td>\n",
              "      <td>0.010776</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.878873</td>\n",
              "      <td>0.453973</td>\n",
              "      <td>0.020195</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.184271</td>\n",
              "      <td>0.865533</td>\n",
              "      <td>0.181675</td>\n",
              "      <td>0.306442</td>\n",
              "      <td>0.021235</td>\n",
              "      <td>0.183804</td>\n",
              "      <td>0.484926</td>\n",
              "      <td>0.974488</td>\n",
              "      <td>0.284444</td>\n",
              "      <td>0.345343</td>\n",
              "      <td>0.148896</td>\n",
              "      <td>0.115320</td>\n",
              "      <td>0.304870</td>\n",
              "      <td>0.304870</td>\n",
              "      <td>0.295238</td>\n",
              "      <td>0.290794</td>\n",
              "      <td>0.128062</td>\n",
              "      <td>0.077815</td>\n",
              "      <td>0.044747</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>0.325508</td>\n",
              "      <td>0.360384</td>\n",
              "      <td>0.045702</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.431175</td>\n",
              "      <td>0.679469</td>\n",
              "      <td>0.172644</td>\n",
              "      <td>0.750469</td>\n",
              "      <td>0.020929</td>\n",
              "      <td>0.380940</td>\n",
              "      <td>0.891717</td>\n",
              "      <td>0.946626</td>\n",
              "      <td>0.707302</td>\n",
              "      <td>0.227592</td>\n",
              "      <td>0.066246</td>\n",
              "      <td>0.109428</td>\n",
              "      <td>0.728653</td>\n",
              "      <td>0.728653</td>\n",
              "      <td>0.729524</td>\n",
              "      <td>0.723810</td>\n",
              "      <td>0.065495</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.049242</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.739531</td>\n",
              "      <td>0.255860</td>\n",
              "      <td>0.042004</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.712849</td>\n",
              "      <td>0.991839</td>\n",
              "      <td>0.240241</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>0.036976</td>\n",
              "      <td>0.700643</td>\n",
              "      <td>0.016835</td>\n",
              "      <td>0.975159</td>\n",
              "      <td>0.268571</td>\n",
              "      <td>0.468366</td>\n",
              "      <td>0.023344</td>\n",
              "      <td>0.249158</td>\n",
              "      <td>0.270715</td>\n",
              "      <td>0.270715</td>\n",
              "      <td>0.269841</td>\n",
              "      <td>0.268571</td>\n",
              "      <td>0.146561</td>\n",
              "      <td>0.020286</td>\n",
              "      <td>0.035455</td>\n",
              "      <td>0.006678</td>\n",
              "      <td>0.267614</td>\n",
              "      <td>0.568813</td>\n",
              "      <td>0.073303</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.338077</td>\n",
              "      <td>0.996782</td>\n",
              "      <td>0.123578</td>\n",
              "      <td>0.045654</td>\n",
              "      <td>0.011389</td>\n",
              "      <td>0.088682</td>\n",
              "      <td>0.219150</td>\n",
              "      <td>0.982544</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.630931</td>\n",
              "      <td>0.100946</td>\n",
              "      <td>0.187710</td>\n",
              "      <td>0.041746</td>\n",
              "      <td>0.041746</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.041905</td>\n",
              "      <td>0.134704</td>\n",
              "      <td>0.011038</td>\n",
              "      <td>0.034491</td>\n",
              "      <td>0.004833</td>\n",
              "      <td>0.071762</td>\n",
              "      <td>0.699979</td>\n",
              "      <td>0.024521</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Solidity  Eccentricity  EquivDiameter  ...  Centroid2      Area  microorganism\n",
              "0  0.711636      0.673498       0.109069  ...   0.453973  0.020195              1\n",
              "1  0.184271      0.865533       0.181675  ...   0.360384  0.045702              2\n",
              "2  0.431175      0.679469       0.172644  ...   0.255860  0.042004              2\n",
              "3  0.712849      0.991839       0.240241  ...   0.568813  0.073303              4\n",
              "4  0.338077      0.996782       0.123578  ...   0.699979  0.024521              2\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0VymVqb8vam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2036da6d-47fc-425d-ae8b-acbb61738daf"
      },
      "source": [
        "mo_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13824 entries, 0 to 13823\n",
            "Data columns (total 24 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Solidity         13824 non-null  float64\n",
            " 1   Eccentricity     13824 non-null  float64\n",
            " 2   EquivDiameter    13824 non-null  float64\n",
            " 3   Extrema          13824 non-null  float64\n",
            " 4   FilledArea       13824 non-null  float64\n",
            " 5   Extent           13824 non-null  float64\n",
            " 6   Orientation      13824 non-null  float64\n",
            " 7   EulerNumber      13824 non-null  float64\n",
            " 8   BoundingBox1     13824 non-null  float64\n",
            " 9   BoundingBox2     13824 non-null  float64\n",
            " 10  BoundingBox3     13824 non-null  float64\n",
            " 11  BoundingBox4     13824 non-null  float64\n",
            " 12  ConvexHull1      13824 non-null  float64\n",
            " 13  ConvexHull2      13824 non-null  float64\n",
            " 14  ConvexHull3      13824 non-null  float64\n",
            " 15  ConvexHull4      13824 non-null  float64\n",
            " 16  MajorAxisLength  13824 non-null  float64\n",
            " 17  MinorAxisLength  13824 non-null  float64\n",
            " 18  Perimeter        13824 non-null  float64\n",
            " 19  ConvexArea       13824 non-null  float64\n",
            " 20  Centroid1        13824 non-null  float64\n",
            " 21  Centroid2        13824 non-null  float64\n",
            " 22  Area             13824 non-null  float64\n",
            " 23  microorganism    13824 non-null  int64  \n",
            "dtypes: float64(23), int64(1)\n",
            "memory usage: 2.5 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJZTl-Hn6SqG"
      },
      "source": [
        "# Separating Input Features and Output Features\n",
        "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we want to predict the class of microorganism (i.e. microorganism), so the microorganism is our target variable and remaining features are input variable.\n",
        "\n",
        "By convention input variables are represented with 'X' and output variables are represented with 'y'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG7oHnzB1_NJ"
      },
      "source": [
        "# Input/independent variables\n",
        "X = mo_data.drop('microorganism', axis = 1)   # her we are droping the microorganism feature as this is the target and 'X' is input features, the changes are not \n",
        "                                              # made inplace as we have not used 'inplace = True'\n",
        "\n",
        "y = mo_data['microorganism']             # Output/Dependent variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydj91s_E64Uu"
      },
      "source": [
        "# Splitting the data into Train and Validation Set\n",
        "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
        "\n",
        "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7EGP5w360BD"
      },
      "source": [
        "# import train_test_split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj5_kJ2Y7FnS"
      },
      "source": [
        "# split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, random_state = 42)\n",
        "\n",
        "# X_train: independent/input feature data for training the model\n",
        "# y_train: dependent/output feature data for training the model\n",
        "# X_test: independent/input feature data for testing the model; will be used to predict the output values\n",
        "# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n",
        " \n",
        "# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n",
        "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the co"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsxciXev7Q_i"
      },
      "source": [
        "# Building Model\n",
        "Now we are finally ready, and we can train the model.\n",
        "\n",
        "There are tons of Machine Learning models like Logistic Regression, Random Forest, Decision Tree, etc. to say you some. However here we are using Logistic Regression (again, using the sklearn library).\n",
        "\n",
        "Then we would feed the model both with the data (X_train) and the answers for that data (y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay4pnyq87LO8"
      },
      "source": [
        "# Importing RandomForestClassifier from sklearn.ensemble\n",
        "# We will be further discussing about why Random Forest is in ensemble module of sklearn library\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6vHN6t763C"
      },
      "source": [
        "lr = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23pEgaC8G8l"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YmXvx3B7-FH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7230d52-7333-4251-f57e-891786e54c22"
      },
      "source": [
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKO513vA8Vhu"
      },
      "source": [
        "# Validate The Model\n",
        "Wonderü§î how well your model learned! Lets check it.\n",
        "\n",
        "### Predict on the validation data (X_val)\n",
        "Now we predict using our trained model on the validation set we created i.e. X_val and evaluate our model on unforeseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d5YdS-W8RD1"
      },
      "source": [
        "pred = lr.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYD_9rcY9xkl"
      },
      "source": [
        "## Model Evaluation\n",
        "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics.\n",
        "\n",
        "There are so many evaluation metrics to use for regression problem, naming some - Accuracy Score, F1 Score, Precision, Recall etc. However, **F1 Score** is the metric for this challenge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4XV0GS-u_r"
      },
      "source": [
        "# import mean squared error from sklearn.metric\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBI7JwY1-40F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278225e3-fcf5-406b-f4d8-baca50091c3e"
      },
      "source": [
        "print('F1 Score is: ', f1_score(y_val, pred, average = 'weighted')) \n",
        "\n",
        "# y_val is the original target value of the validation set (X_val)\n",
        "# pred is the predicted target value of the validation set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score is:  0.7759095931897805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1C_HVx_lKV"
      },
      "source": [
        "# Predict The Output For Testing Dataset üòÖ\n",
        "We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. testing_set_label.csv) given in 'Data' section of the problem page.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BlJRzftAjTO"
      },
      "source": [
        "## Load Test Set\n",
        "Load the test data on which final submission is to be made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LHXg8ZAilM"
      },
      "source": [
        "test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/sukhna_dhanas/test_set_label.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSlXRaJRA2K9"
      },
      "source": [
        "**Note:** \n",
        "*  Use the same techniques to deal with missing values as done with the training dataset.   \n",
        "\n",
        "*  **Don't remove any observation/record from the test dataset otherwise you will get wrong answer. The number of items in your prediction should be same as the number of records are present in the test dataset**.\n",
        "\n",
        "*  Use the same techniques to preprocess the data as done with training dataset.\n",
        "\n",
        "***Why do we need to do the same procedure of filling missing values, data cleaning and data preprocessing on the new test data as it was done for the training and validation data?***\n",
        "\n",
        "**Ans:** Because our model has been trained on certain format of data and if we don't provide the testing data of the similar format, the model will give erroneous predictions and the rmse of the model will increase. Also, if the model was build on 'n' number of features, while predicting on new test data you should always give the same number of features to the model. In this case if you provide different number of features while predicting the output, your ML model will throw a ValueError saying something like 'number of features given x; expecting n'. Not confident about these statements? Well, as a data scientist you should always perform some experiment and observe the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPUaiH-Zyhj_",
        "outputId": "40ab0d9a-f0ba-4e69-e5c1-06580922a2e7"
      },
      "source": [
        "test_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3456 entries, 0 to 3455\n",
            "Data columns (total 23 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Solidity         3456 non-null   float64\n",
            " 1   Eccentricity     3456 non-null   float64\n",
            " 2   EquivDiameter    3456 non-null   float64\n",
            " 3   Extrema          3456 non-null   float64\n",
            " 4   FilledArea       3456 non-null   float64\n",
            " 5   Extent           3456 non-null   float64\n",
            " 6   Orientation      3456 non-null   float64\n",
            " 7   EulerNumber      3456 non-null   float64\n",
            " 8   BoundingBox1     3456 non-null   float64\n",
            " 9   BoundingBox2     3456 non-null   float64\n",
            " 10  BoundingBox3     3456 non-null   float64\n",
            " 11  BoundingBox4     3456 non-null   float64\n",
            " 12  ConvexHull1      3456 non-null   float64\n",
            " 13  ConvexHull2      3456 non-null   float64\n",
            " 14  ConvexHull3      3456 non-null   float64\n",
            " 15  ConvexHull4      3456 non-null   float64\n",
            " 16  MajorAxisLength  3456 non-null   float64\n",
            " 17  MinorAxisLength  3456 non-null   float64\n",
            " 18  Perimeter        3456 non-null   float64\n",
            " 19  ConvexArea       3456 non-null   float64\n",
            " 20  Centroid1        3456 non-null   float64\n",
            " 21  Centroid2        3456 non-null   float64\n",
            " 22  Area             3456 non-null   float64\n",
            "dtypes: float64(23)\n",
            "memory usage: 621.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1VWI-oiCp3t"
      },
      "source": [
        "## Make Prediction on Test Dataset\n",
        "Time to make submission!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Fsfy9iA1N7"
      },
      "source": [
        "target = lr.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrNaxq9KEp0n"
      },
      "source": [
        "#### Note: **Follow the submission guidelines given in 'How To Submit' Section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2U3r7CC99s"
      },
      "source": [
        "## How to save prediciton results locally via jupyter notebook?\n",
        "If you are working on Jupyter notebook, execute below block of codes. A file named 'prediction_results.csv' will be created in your current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyT2vAGIDjnF"
      },
      "source": [
        "res = pd.DataFrame(target) #target is nothing but the final predictions of your model on input features of your new unseen test data\n",
        "res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n",
        "res.columns = [\"prediction\"]\n",
        "res.to_csv(\"submission.csv\")      # the csv file will be saved locally on the same location where this notebook is located."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98_QXo0TD_0G"
      },
      "source": [
        "# **OR**, \n",
        "**if you are working on Google Colab then use the below set of code to save prediction results locally**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrE2CjoDwje"
      },
      "source": [
        "## How to save prediction results locally via colab notebook?\n",
        "If you are working on Google Colab Notebook, execute below block of codes. A file named 'prediction_results' will be downloaded in your system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgcOUpjKDzzI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a29a65aa-bb9b-4167-e4e7-ccacdb0a0ee8"
      },
      "source": [
        "# To create Dataframe of predicted value with particular respective index\n",
        "res = pd.DataFrame(target) # target are nothing but the final predictions of your model on input features of your new unseen test data\n",
        "res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n",
        "res.columns = [\"prediction\"]\n",
        "\n",
        "# To download the csv file locally\n",
        "from google.colab import files\n",
        "res.to_csv('submission.csv')         \n",
        "files.download('submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f1e125b1-d5b1-4577-81ab-c868392c2cb2\", \"submission.csv\", 23094)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghEzJ8zE4kM"
      },
      "source": [
        "## **Well Done! üëç**\n",
        "You are all set to make a submission. Let's head to the [challenge page](https://dphi.tech/practice/challenge/55) to make the submission."
      ]
    }
  ]
}